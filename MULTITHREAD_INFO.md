# 멀티스레드 크롤링 가이드

## 변경 사항

크롤링이 **멀티스레드**로 병렬 처리되도록 수정되었습니다.

## 주요 개선사항

### 1. 병렬 처리
- 여러 제품을 동시에 크롤링
- 기본 워커 수: **3개** (안정성 기준)
- 최대 워커 수: **10개**

### 2. 속도 향상
- **예상 속도**: 기존 대비 약 **3-5배** 빠름
- 100개 제품 기준: 약 2-3분 → **40-60초** (워커 3개 기준)

### 3. 스레드 안전성
- 진행률 업데이트는 `threading.Lock`으로 보호
- 결과 저장도 스레드 안전하게 처리

## 워커 수 설정

### 환경 변수로 설정
```bash
# Windows PowerShell
$env:CRAWLER_WORKERS = "5"

# Linux/Mac
export CRAWLER_WORKERS=5
```

### 코드에서 설정
```python
crawler = PriceCompareCrawler(config_file="ssg_input_list.jsonl", site_name="ssg")
crawler.run_crawling(max_workers=5)  # 워커 수 직접 지정
```

### 권장 워커 수
- **소규모 (10-50개 제품)**: 3개
- **중규모 (50-200개 제품)**: 5개
- **대규모 (200개 이상)**: 7~10개 (서버 사양에 따라 조정)

## 주의사항

### 1. 서버 부하
- 워커 수가 많을수록 서버 리소스 사용량 증가
- CPU와 메모리 사용량 모니터링 권장

### 2. 타겟 사이트 제한
- 일부 사이트는 과도한 요청 시 IP 차단 가능
- Rate Limiting 고려 필요
- 현재 요청 간격: **0.5초** (기존 1초에서 단축)

### 3. 네트워크 대역폭
- 동시 요청이 많을수록 네트워크 사용량 증가
- 네트워크 환경에 따라 조정 필요

## 성능 비교

### 기존 (순차 처리)
```
제품 1 → 제품 2 → 제품 3 → ...
100개 제품: 약 2-3분
```

### 멀티스레드 (3개 워커 기준)
```
제품 1, 2, 3 (동시)
→ 제품 4, 5, 6 (동시)
→ ...
100개 제품: 약 40-60초
```

## 에러 처리

- 일부 제품 크롤링 실패해도 전체 작업 계속 진행
- 실패한 제품은 `error` 필드에 에러 메시지 저장
- 최종 결과 파일에는 모든 제품 포함

## 모니터링

크롤링 시작 시 다음 정보가 출력됩니다:
```
=== 멀티스레드 크롤링 시작 ===
전체 제품 수: 100
워커 수: 3
예상 속도 향상: 약 3배
```

진행 중:
```
[14%] 크롤링 완료 (14/100): 제품명
[21%] 크롤링 완료 (21/100): 제품명
...
```

## 배포 환경 설정

### Render
```bash
# Environment Variables
CRAWLER_WORKERS=3
```

### 로컬 개발
- 환경 변수 설정 없이 기본값(3개) 사용
- 필요시 코드에서 직접 지정

## 문제 해결

### 크롤링이 너무 느린 경우
- 워커 수 증가 (최대 10개)
- 네트워크 상태 확인
- 타겟 사이트 응답 속도 확인

### 서버 부하가 높은 경우
- 워커 수 감소 (5개로 조정)
- `CRAWLER_WORKERS` 환경 변수로 조정

### IP 차단되는 경우
- 워커 수 감소
- 요청 간격 증가 (`time.sleep` 값 증가)
- 타겟 사이트 정책 확인

